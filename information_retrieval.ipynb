{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "This notebook contains some experiments with information retrieval.\n",
    "Topics:\n",
    "* indexing\n",
    "* search\n",
    "* result scoring: TF-IDF, vector space models, term frequency counts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import hashlib\n",
    "import urllib.request\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shakespeare's sonnets\"\"\"\n",
    "\n",
    "ROMAN_NUMERAL = re.compile(r'^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$')\n",
    "MD5_DIGEST = '83ad11c60641bb60d44fb75c406c0c9b'\n",
    "\n",
    "def download_sonnets():\n",
    "    raw = urllib.request.urlopen('https://www.gutenberg.org/cache/epub/1041/pg1041.txt').read()\n",
    "    assert hashlib.md5(raw).hexdigest() == MD5_DIGEST, \"md5 does not match\"\n",
    "    return raw.decode('utf-8')\n",
    "\n",
    "def extract_sonnet(lines, start):\n",
    "    num = lines[start].strip()\n",
    "    title = f'Sonnet {num}'\n",
    "    content = []\n",
    "    for pos in range(start+2, len(lines)):\n",
    "        line = lines[pos].strip()\n",
    "        if not line:\n",
    "            break\n",
    "        content.append(line)\n",
    "    return title, \"\\n\".join(content)\n",
    "\n",
    "def extract_sonnets(content):\n",
    "    lines = content.split('\\n')\n",
    "    starts = [pos for pos, line in enumerate(lines) if line.strip() and ROMAN_NUMERAL.match(line.strip())]\n",
    "    sonnets = []\n",
    "    for start in starts:\n",
    "        sonnets.append(extract_sonnet(lines, start))\n",
    "    return sonnets\n",
    "\n",
    "# [(title, content)]\n",
    "sonnets = extract_sonnets(download_sonnets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words for document generation\n",
    "words = open('/usr/share/dict/words').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Doc generation\"\"\"\n",
    "def sentence(n): return ' '.join(random.choice(words) for _ in range(n)) + '.'\n",
    "def content(n): return ' '.join(sentence(10) for _ in range(n // 10))\n",
    "def mktitle(): return ' '.join(random.choice(words) for _ in range(5))\n",
    "def mkauthor(): return random.choice(words) + ' ' + random.choice(words)\n",
    "def mkdate(): return datetime.utcnow()\n",
    "gdocids = itertools.count()\n",
    "def mkdoc(**fields): return {\n",
    "    'docid': next(gdocids),\n",
    "    'title': mktitle(),\n",
    "    'author': mkauthor(),\n",
    "    'date': mkdate(),\n",
    "    'content': content(random.randint(64, 128)),\n",
    "    **fields\n",
    "}\n",
    "def mkdocs(n): return [mkdoc() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_docs = [mkdoc(title=title, content=content, author='shakespeare') for title, content in sonnets]\n",
    "rand_docs = mkdocs(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Indexing\"\"\"\n",
    "\n",
    "def tokenize(s): return (tok.lower() for tok in re.findall(r'\\w+', s))\n",
    "\n",
    "def index_doc(doc):\n",
    "    index = defaultdict(list)\n",
    "    for pos, term in enumerate(tokenize(doc['content'])):\n",
    "        index[term].append(pos)\n",
    "    return index\n",
    "\n",
    "def rev_index(docs):\n",
    "    index = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        for term in tokenize(doc['content']):\n",
    "            postings = index[term]\n",
    "            if not postings or postings[-1] != doc['docid']:\n",
    "                postings.append(doc['docid'])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_rindex = rev_index(sonnet_docs)\n",
    "sonnet_doc_indexes = [index_doc(doc) for doc in sonnet_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Searching\"\"\"\n",
    "import bisect\n",
    "\n",
    "def isect(left, right):\n",
    "    \"\"\"Intersect two sorted iterables\"\"\"\n",
    "    left, right = iter(left), iter(right)\n",
    "    l, r = next(left, None), next(right, None)\n",
    "    while l is not None and r is not None:\n",
    "        if l == r:\n",
    "            yield l\n",
    "            l, r = next(left, None), next(right, None)\n",
    "        elif l < r:\n",
    "            l = next(left, None)\n",
    "        else:\n",
    "            r = next(right, None)\n",
    "            \n",
    "def isectn(*lists):\n",
    "    \"\"\"Intersect N sorted iterables\"\"\"\n",
    "    it = iter(lists)\n",
    "    res = next(it, [])\n",
    "    for xs in it:\n",
    "        res = isect(res, xs)\n",
    "    return res\n",
    "    \n",
    "def exact_search(docindex, query):\n",
    "    \"\"\"Returns [term_offset] of exact matches\"\"\"\n",
    "    if not query: \n",
    "        return []\n",
    "    hits = [docindex[term] for term in query]\n",
    "    idx = 0\n",
    "    results = []\n",
    "    while idx < len(hits[0]):\n",
    "        offset = hits[0][idx]\n",
    "        next_offset = None\n",
    "        for j in range(1, len(query)):\n",
    "            jhits = hits[j]\n",
    "            jidx = bisect.bisect_left(jhits, offset)\n",
    "            if jidx == len(jhits):\n",
    "                return results\n",
    "            joffset = jhits[jidx]\n",
    "            if joffset != offset + j:\n",
    "                next_offset = joffset - j\n",
    "                break\n",
    "        if next_offset is None:\n",
    "            results.append(offset)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx = bisect.bisect_left(hits[0], next_offset)\n",
    "    return results\n",
    "    \n",
    "    \n",
    "def search(rindex, query):\n",
    "    doclists = [rindex[term] for term in query]\n",
    "    return list(isectn(*doclists))\n",
    "\n",
    "def parse_query(query):\n",
    "    return list(tokenize(query))\n",
    "\n",
    "def pretty_search(docs, rindex, query):\n",
    "    titles = {doc['docid']: doc['title'] for doc in docs}\n",
    "    results = search(rindex, parse_query(query))\n",
    "    return [titles[docid] for docid in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A few tests never hurt\"\"\"\n",
    "assert list(isect([0, 1, 2], [-1, 1, 3])) == [1]\n",
    "assert list(isectn([0, 1, 2], [1], [-1, 1, 2, 5])) == [1]\n",
    "rindex = {\n",
    "    'a': [0, 1, 2],\n",
    "    'b': [1],\n",
    "    'c': [1, 2],\n",
    "}\n",
    "assert search(rindex, ['a']) == [0, 1, 2]\n",
    "assert search(rindex, ['a', 'c']) == [1, 2], search(rindex, ['a', 'c'])\n",
    "assert search(rindex, ['a', 'b', 'c']) == [1]\n",
    "\n",
    "def test_exact_search():\n",
    "    sample = \"\"\"\n",
    "jack and jill ran up the hill\n",
    "but no one stopped to pay the bill\n",
    "up the hill up the jack the hill jack and jill\n",
    "\"\"\"\n",
    "    queries = [\n",
    "        'jack',\n",
    "        'jack and jill',\n",
    "        'no one stopped',\n",
    "        'up the hill',\n",
    "        'FOO BAR'\n",
    "    ]\n",
    "    tokens = list(tokenize(sample))\n",
    "    dindex = index_doc(mkdoc(content=sample))\n",
    "    for q in queries:\n",
    "        pq = parse_query(q)\n",
    "        expect = [offset for offset, _ in enumerate(tokens) if tokens[offset:offset+len(pq)] == pq]\n",
    "        assert exact_search(dindex, pq) == expect\n",
    "\n",
    "test_exact_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairest creatures: ['Sonnet I']\n",
      "her beauty: ['Sonnet IX', 'Sonnet XI', 'Sonnet XIX', 'Sonnet XXII', 'Sonnet XLI', 'Sonnet CXXVII']\n",
      "spring: ['Sonnet I', 'Sonnet LIII', 'Sonnet LXIII', 'Sonnet XCVIII', 'Sonnet CII']\n",
      "my love: ['Sonnet X', 'Sonnet XIII', 'Sonnet XV', 'Sonnet XIX', 'Sonnet XX', 'Sonnet XXI', 'Sonnet XXII', 'Sonnet XXIII', 'Sonnet XXVI', 'Sonnet XXIX', 'Sonnet XXX', 'Sonnet XXXI', 'Sonnet XXXII', 'Sonnet XXXIII', 'Sonnet XXXIV', 'Sonnet XXXV', 'Sonnet XXXVI', 'Sonnet XXXVII', 'Sonnet XL', 'Sonnet XLII', 'Sonnet XLV', 'Sonnet XLVI', 'Sonnet XLVII', 'Sonnet XLIX', 'Sonnet LI', 'Sonnet LVII', 'Sonnet LXI', 'Sonnet LXII', 'Sonnet LXIII', 'Sonnet LXIV', 'Sonnet LXV', 'Sonnet LXVI', 'Sonnet LXXI', 'Sonnet LXXII', 'Sonnet LXXVI', 'Sonnet LXXIX', 'Sonnet LXXX', 'Sonnet LXXXII', 'Sonnet LXXXV', 'Sonnet LXXXVIII', 'Sonnet LXXXIX', 'Sonnet XCI', 'Sonnet XCII', 'Sonnet XCIX', 'Sonnet C', 'Sonnet CI', 'Sonnet CII', 'Sonnet CV', 'Sonnet CVII', 'Sonnet CVIII', 'Sonnet CIX', 'Sonnet CX', 'Sonnet CXII', 'Sonnet CXIV', 'Sonnet CXV', 'Sonnet CXVII', 'Sonnet CXVIII', 'Sonnet CXIX', 'Sonnet CXXII', 'Sonnet CXXIV', 'Sonnet CXXX', 'Sonnet CXXXI', 'Sonnet CXXXII', 'Sonnet CXXXVI', 'Sonnet CXXXVII', 'Sonnet CXXXVIII', 'Sonnet CXXXIX', 'Sonnet CXL', 'Sonnet CXLI', 'Sonnet CXLII', 'Sonnet CXLV', 'Sonnet CXLVII', 'Sonnet CXLVIII', 'Sonnet CXLIX', 'Sonnet CL', 'Sonnet CLI', 'Sonnet CLII', 'Sonnet CLIII', 'Sonnet CLIV']\n",
      "tender: ['Sonnet I', 'Sonnet XXII', 'Sonnet XLV', 'Sonnet LXXXIII', 'Sonnet CXX', 'Sonnet CXXVIII', 'Sonnet CXLI']\n",
      "thine own: ['Sonnet I', 'Sonnet II', 'Sonnet XIX', 'Sonnet XXXVIII', 'Sonnet XLIX', 'Sonnet LXIX', 'Sonnet CXLII']\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"fairest creatures\",\n",
    "    \"her beauty\",\n",
    "    \"spring\",\n",
    "    \"my love\",\n",
    "    \"tender\",\n",
    "    \"thine own\",\n",
    "]\n",
    "for q in queries:\n",
    "    print(f\"{q}: {pretty_search(sonnet_docs, sonnet_rindex, q)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TF-IDF\"\"\"\n",
    "\n",
    "def docfreqs(docs):\n",
    "    docfreqs = Counter()\n",
    "    for doc in docs:\n",
    "        for term in set(tokenize(doc['content'])):\n",
    "            docfreqs[term] += 1\n",
    "    return docfreqs\n",
    "\n",
    "def termfreqs(doc):\n",
    "    return Counter(tokenize(doc['content']))\n",
    "    \n",
    "def tfidf(docs):\n",
    "    df = docfreqs(docs)\n",
    "    for doc in docs:\n",
    "        scores = {}\n",
    "        for term, tf in termfreqs(doc).items():\n",
    "            idf = math.log(len(docs) / df[term], 2)\n",
    "            tfidf = tf * idf\n",
    "            scores[term] = tfidf\n",
    "        yield scores\n",
    "        \n",
    "def tfidf_table(docs):\n",
    "    return {doc['docid']: scores for doc, scores in zip(docs, tfidf(docs))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
