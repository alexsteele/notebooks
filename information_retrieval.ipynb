{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "This notebook contains some experiments with information retrieval.\n",
    "Topics:\n",
    "* indexing\n",
    "* search\n",
    "* result scoring: TF-IDF, vector space models, term frequency counts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import hashlib\n",
    "import urllib.request\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shakespeare's sonnets\"\"\"\n",
    "\n",
    "ROMAN_NUMERAL = re.compile(r'^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$')\n",
    "MD5_DIGEST = '83ad11c60641bb60d44fb75c406c0c9b'\n",
    "\n",
    "def download_sonnets():\n",
    "    raw = urllib.request.urlopen('https://www.gutenberg.org/cache/epub/1041/pg1041.txt').read()\n",
    "    assert hashlib.md5(raw).hexdigest() == MD5_DIGEST, \"md5 does not match\"\n",
    "    return raw.decode('utf-8')\n",
    "\n",
    "def extract_sonnet(lines, start):\n",
    "    num = lines[start].strip()\n",
    "    title = f'Sonnet {num}'\n",
    "    content = []\n",
    "    for pos in range(start+2, len(lines)):\n",
    "        line = lines[pos].strip()\n",
    "        if not line:\n",
    "            break\n",
    "        content.append(line)\n",
    "    return title, \"\\n\".join(content)\n",
    "\n",
    "def extract_sonnets(content):\n",
    "    lines = content.split('\\n')\n",
    "    starts = [pos for pos, line in enumerate(lines) if line.strip() and ROMAN_NUMERAL.match(line.strip())]\n",
    "    sonnets = []\n",
    "    for start in starts:\n",
    "        sonnets.append(extract_sonnet(lines, start))\n",
    "    return sonnets\n",
    "\n",
    "# [(title, content)]\n",
    "sonnets = extract_sonnets(download_sonnets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words for document generation\n",
    "words = open('/usr/share/dict/words').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Doc generation\"\"\"\n",
    "def sentence(n): return ' '.join(random.choice(words) for _ in range(n)) + '.'\n",
    "def content(n): return ' '.join(sentence(10) for _ in range(n // 10))\n",
    "def mktitle(): return ' '.join(random.choice(words) for _ in range(5))\n",
    "def mkauthor(): return random.choice(words) + ' ' + random.choice(words)\n",
    "def mkdate(): return datetime.utcnow()\n",
    "gdocids = itertools.count()\n",
    "def mkdoc(**fields): return {\n",
    "    'docid': next(gdocids),\n",
    "    'title': mktitle(),\n",
    "    'author': mkauthor(),\n",
    "    'date': mkdate(),\n",
    "    'content': content(random.randint(64, 128)),\n",
    "    **fields\n",
    "}\n",
    "def mkdocs(n): return [mkdoc() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_docs = [mkdoc(title=title, content=content, author='shakespeare') for title, content in sonnets]\n",
    "rand_docs = mkdocs(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Indexing\"\"\"\n",
    "\n",
    "def tokenize(s): return (tok.lower() for tok in re.findall(r'\\w+', s))\n",
    "\n",
    "def index_doc(doc):\n",
    "    \"\"\"Returns {term: [offset]}\"\"\"\n",
    "    index = defaultdict(list)\n",
    "    for pos, term in enumerate(tokenize(doc['content'])):\n",
    "        index[term].append(pos)\n",
    "    return index\n",
    "\n",
    "def rev_index(docs):\n",
    "    \"\"\"Returns {term: [docid]}\"\"\"\n",
    "    index = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        for term in tokenize(doc['content']):\n",
    "            postings = index[term]\n",
    "            if not postings or postings[-1] != doc['docid']:\n",
    "                postings.append(doc['docid'])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_rindex = rev_index(sonnet_docs)\n",
    "sonnet_doc_indexes = [index_doc(doc) for doc in sonnet_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Searching\"\"\"\n",
    "import bisect\n",
    "\n",
    "def isect(left, right):\n",
    "    \"\"\"Intersect two sorted iterables\"\"\"\n",
    "    left, right = iter(left), iter(right)\n",
    "    l, r = next(left, None), next(right, None)\n",
    "    while l is not None and r is not None:\n",
    "        if l == r:\n",
    "            yield l\n",
    "            l, r = next(left, None), next(right, None)\n",
    "        elif l < r:\n",
    "            l = next(left, None)\n",
    "        else:\n",
    "            r = next(right, None)\n",
    "            \n",
    "def isectn(*lists):\n",
    "    \"\"\"Intersect N sorted iterables\"\"\"\n",
    "    it = iter(lists)\n",
    "    res = next(it, [])\n",
    "    for xs in it:\n",
    "        res = isect(res, xs)\n",
    "    return res\n",
    "    \n",
    "def exact_search(docindex, query):\n",
    "    \"\"\"Returns [term_offset] of exact matches\"\"\"\n",
    "    if not query: \n",
    "        return []\n",
    "    hits = [docindex[term] for term in query]\n",
    "    idx = 0\n",
    "    results = []\n",
    "    while idx < len(hits[0]):\n",
    "        offset = hits[0][idx]\n",
    "        next_offset = None\n",
    "        for j in range(1, len(query)):\n",
    "            jhits = hits[j]\n",
    "            jidx = bisect.bisect_left(jhits, offset)\n",
    "            if jidx == len(jhits):\n",
    "                return results\n",
    "            joffset = jhits[jidx]\n",
    "            if joffset != offset + j:\n",
    "                next_offset = joffset - j\n",
    "                break\n",
    "        if next_offset is None:\n",
    "            results.append(offset)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx = bisect.bisect_left(hits[0], next_offset)\n",
    "    return results\n",
    "    \n",
    "    \n",
    "def search(rindex, query):\n",
    "    doclists = [rindex[term] for term in query]\n",
    "    return list(isectn(*doclists))\n",
    "\n",
    "def parse_query(query):\n",
    "    return list(tokenize(query))\n",
    "\n",
    "def pretty_search(docs, rindex, query):\n",
    "    titles = {doc['docid']: doc['title'] for doc in docs}\n",
    "    results = search(rindex, parse_query(query))\n",
    "    return [titles[docid] for docid in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A few tests never hurt\"\"\"\n",
    "assert list(isect([0, 1, 2], [-1, 1, 3])) == [1]\n",
    "assert list(isectn([0, 1, 2], [1], [-1, 1, 2, 5])) == [1]\n",
    "rindex = {\n",
    "    'a': [0, 1, 2],\n",
    "    'b': [1],\n",
    "    'c': [1, 2],\n",
    "}\n",
    "assert search(rindex, ['a']) == [0, 1, 2]\n",
    "assert search(rindex, ['a', 'c']) == [1, 2], search(rindex, ['a', 'c'])\n",
    "assert search(rindex, ['a', 'b', 'c']) == [1]\n",
    "\n",
    "def test_exact_search():\n",
    "    sample = \"\"\"\n",
    "jack and jill ran up the hill\n",
    "but no one stopped to pay the bill\n",
    "up the hill up the jack the hill jack and jill\n",
    "\"\"\"\n",
    "    queries = [\n",
    "        'jack',\n",
    "        'jack and jill',\n",
    "        'no one stopped',\n",
    "        'up the hill',\n",
    "        'FOO BAR'\n",
    "    ]\n",
    "    tokens = list(tokenize(sample))\n",
    "    dindex = index_doc(mkdoc(content=sample))\n",
    "    for q in queries:\n",
    "        pq = parse_query(q)\n",
    "        expect = [offset for offset, _ in enumerate(tokens) if tokens[offset:offset+len(pq)] == pq]\n",
    "        assert exact_search(dindex, pq) == expect\n",
    "\n",
    "test_exact_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairest creatures: ['Sonnet I']\n",
      "her beauty: ['Sonnet IX', 'Sonnet XI', 'Sonnet XIX', 'Sonnet XXII', 'Sonnet XLI', 'Sonnet CXXVII']\n",
      "spring: ['Sonnet I', 'Sonnet LIII', 'Sonnet LXIII', 'Sonnet XCVIII', 'Sonnet CII']\n",
      "tender: ['Sonnet I', 'Sonnet XXII', 'Sonnet XLV', 'Sonnet LXXXIII', 'Sonnet CXX', 'Sonnet CXXVIII', 'Sonnet CXLI']\n",
      "thine own: ['Sonnet I', 'Sonnet II', 'Sonnet XIX', 'Sonnet XXXVIII', 'Sonnet XLIX', 'Sonnet LXIX', 'Sonnet CXLII']\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"fairest creatures\",\n",
    "    \"her beauty\",\n",
    "    \"spring\",\n",
    "    \"tender\",\n",
    "    \"thine own\",\n",
    "]\n",
    "for q in queries:\n",
    "    print(f\"{q}: {pretty_search(sonnet_docs, sonnet_rindex, q)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scoring\"\"\"\n",
    "\n",
    "def docfreqs(docs):\n",
    "    docfreqs = Counter()\n",
    "    for doc in docs:\n",
    "        for term in set(tokenize(doc['content'])):\n",
    "            docfreqs[term] += 1\n",
    "    return docfreqs\n",
    "\n",
    "def termfreqs(doc):\n",
    "    return Counter(tokenize(doc['content']))\n",
    "    \n",
    "def tfidf(docs):\n",
    "    df = docfreqs(docs)\n",
    "    for doc in docs:\n",
    "        scores = {}\n",
    "        for term, tf in termfreqs(doc).items():\n",
    "            idf = math.log(len(docs) / df[term], 2)\n",
    "            tfidf = tf * idf\n",
    "            scores[term] = tfidf\n",
    "        yield scores\n",
    "        \n",
    "def tfidf_table(docs):\n",
    "    return {doc['docid']: scores for doc, scores in zip(docs, tfidf(docs))}\n",
    "\n",
    "def score_tfidf(doc, query):\n",
    "    pass  # TODO: implement\n",
    "   \n",
    "def score_freq(doc_index, query):\n",
    "    return sum(len(doc_index[term]) for term in query)\n",
    "\n",
    "def close_hits(doc_index, query, threshold):\n",
    "    hits = [doc_index[term] for term in query]\n",
    "    for idx in range(len(hits[0])):\n",
    "        offset = hits[0][idx]\n",
    "        indices = [idx] + [bisect.bisect_left(hits[j], offset) for j in range(1, len(hits))]\n",
    "        offsets = [hits[j][idx] if idx < len(hits[j]) else -1 for j, idx in enumerate(indices)]\n",
    "        for j in range(1, len(offsets)):\n",
    "            if offsets[j] == -1 or offsets[j-1] == -1:\n",
    "                continue\n",
    "            distance = offsets[j] - offsets[j-1]\n",
    "            if distance <= threshold:\n",
    "                yield distance\n",
    "    \n",
    "def score_proximity(doc_index, query, prox_bins=None, count_bins=None):\n",
    "    if not query:\n",
    "        return 0.0\n",
    "    prox_bins = prox_bins or [(0, 1.0), (1, 0.9), (2, 0.8), (3, 0.7), (4, 0.6), (5, 0.5)]\n",
    "    count_bins = count_bins or [(0, 0.0), (1, 0.5), (2, 0.75), (3, 0.85), (4, 0.95), (5, 1.0)]\n",
    "    prox_counts = [0 for _ in prox_bins]\n",
    "    threshold = max(prox for prox, weight in prox_bins)\n",
    "    for distance in close_hits(doc_index, query, threshold=threshold):\n",
    "        binidx = next(i for i in range(len(prox_bins)) if distance <= prox_bins[i][0])\n",
    "        prox_counts[binidx] += 1\n",
    "    prox_weights = [weight for prox, weight in prox_bins]\n",
    "    count_weights = [next((weight for threshold, weight in count_bins if count <= threshold), 0.0)\n",
    "                    for count in prox_counts]\n",
    "    return np.dot(prox_weights, count_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "2.475, 0.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6ce63126b27a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{score}, {expect}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcheck_proximity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a b c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a b c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-6ce63126b27a>\u001b[0m in \u001b[0;36mcheck_proximity\u001b[0;34m(content, query, expect)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_proximity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{score}, {expect}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcheck_proximity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a b c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a b c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 2.475, 0.0"
     ]
    }
   ],
   "source": [
    "def check_proximity(content, query, expect):\n",
    "    doc = mkdoc(content=content)\n",
    "    query = parse_query(query)\n",
    "    score = score_proximity(index_doc(doc), query)\n",
    "    assert score == expect, f\"{score}, {expect}\"\n",
    "    \n",
    "check_proximity(\"a b c\", \"a b c\", 0.0)  # FIXME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
