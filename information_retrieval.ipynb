{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "This notebook contains some experiments with information retrieval.\n",
    "Topics:\n",
    "* indexing\n",
    "* search\n",
    "* result scoring: TF-IDF, vector space models, term frequency counts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import hashlib\n",
    "import urllib.request\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shakespeare's sonnets\"\"\"\n",
    "\n",
    "ROMAN_NUMERAL = re.compile(r'^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$')\n",
    "MD5_DIGEST = '83ad11c60641bb60d44fb75c406c0c9b'\n",
    "\n",
    "def download_sonnets():\n",
    "    raw = urllib.request.urlopen('https://www.gutenberg.org/cache/epub/1041/pg1041.txt').read()\n",
    "    assert hashlib.md5(raw).hexdigest() == MD5_DIGEST, \"md5 does not match\"\n",
    "    return raw.decode('utf-8')\n",
    "\n",
    "def extract_sonnet(lines, start):\n",
    "    num = lines[start].strip()\n",
    "    title = f'Sonnet {num}'\n",
    "    content = []\n",
    "    for pos in range(start+2, len(lines)):\n",
    "        line = lines[pos].strip()\n",
    "        if not line:\n",
    "            break\n",
    "        content.append(line)\n",
    "    return title, \"\\n\".join(content)\n",
    "\n",
    "def extract_sonnets(content):\n",
    "    lines = content.split('\\n')\n",
    "    starts = [pos for pos, line in enumerate(lines) if line.strip() and ROMAN_NUMERAL.match(line.strip())]\n",
    "    sonnets = []\n",
    "    for start in starts:\n",
    "        sonnets.append(extract_sonnet(lines, start))\n",
    "    return sonnets\n",
    "\n",
    "# [(title, content)]\n",
    "sonnets = extract_sonnets(download_sonnets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words for document generation\n",
    "words = open('/usr/share/dict/words').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Doc generation\"\"\"\n",
    "def sentence(n): return ' '.join(random.choice(words) for _ in range(n)) + '.'\n",
    "def content(n): return ' '.join(sentence(10) for _ in range(n // 10))\n",
    "def mktitle(): return ' '.join(random.choice(words) for _ in range(5))\n",
    "def mkauthor(): return random.choice(words) + ' ' + random.choice(words)\n",
    "def mkdate(): return datetime.utcnow()\n",
    "gdocids = itertools.count()\n",
    "def mkdoc(**fields): return {\n",
    "    'docid': next(gdocids),\n",
    "    'title': mktitle(),\n",
    "    'author': mkauthor(),\n",
    "    'date': mkdate(),\n",
    "    'content': content(random.randint(64, 128)),\n",
    "    **fields\n",
    "}\n",
    "def mkdocs(n): return [mkdoc() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_docs = [mkdoc(title=title, content=content, author='shakespeare') for title, content in sonnets]\n",
    "rand_docs = mkdocs(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Indexing\"\"\"\n",
    "\n",
    "def tokenize(s): return (tok.lower() for tok in re.findall(r'\\w+', s))\n",
    "\n",
    "def index_doc(doc):\n",
    "    \"\"\"Returns {term: [offset]}\"\"\"\n",
    "    index = defaultdict(list)\n",
    "    for pos, term in enumerate(tokenize(doc['content'])):\n",
    "        index[term].append(pos)\n",
    "    return index\n",
    "\n",
    "def rev_index(docs):\n",
    "    \"\"\"Returns {term: [docid]}\"\"\"\n",
    "    index = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        for term in tokenize(doc['content']):\n",
    "            postings = index[term]\n",
    "            if not postings or postings[-1] != doc['docid']:\n",
    "                postings.append(doc['docid'])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_rindex = rev_index(sonnet_docs)\n",
    "sonnet_doc_indexes = [index_doc(doc) for doc in sonnet_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Searching\"\"\"\n",
    "import bisect\n",
    "\n",
    "def isect(left, right):\n",
    "    \"\"\"Intersect two sorted iterables\"\"\"\n",
    "    left, right = iter(left), iter(right)\n",
    "    l, r = next(left, None), next(right, None)\n",
    "    while l is not None and r is not None:\n",
    "        if l == r:\n",
    "            yield l\n",
    "            l, r = next(left, None), next(right, None)\n",
    "        elif l < r:\n",
    "            l = next(left, None)\n",
    "        else:\n",
    "            r = next(right, None)\n",
    "            \n",
    "def isectn(*lists):\n",
    "    \"\"\"Intersect N sorted iterables\"\"\"\n",
    "    it = iter(lists)\n",
    "    res = next(it, [])\n",
    "    for xs in it:\n",
    "        res = isect(res, xs)\n",
    "    return res\n",
    "    \n",
    "def exact_search(docindex, query):\n",
    "    \"\"\"Returns [term_offset] of exact matches\"\"\"\n",
    "    if not query: \n",
    "        return []\n",
    "    hits = [docindex[term] for term in query]\n",
    "    idx = 0\n",
    "    results = []\n",
    "    while idx < len(hits[0]):\n",
    "        offset = hits[0][idx]\n",
    "        next_offset = None\n",
    "        for j in range(1, len(query)):\n",
    "            jhits = hits[j]\n",
    "            jidx = bisect.bisect_left(jhits, offset)\n",
    "            if jidx == len(jhits):\n",
    "                return results\n",
    "            joffset = jhits[jidx]\n",
    "            if joffset != offset + j:\n",
    "                next_offset = joffset - j\n",
    "                break\n",
    "        if next_offset is None:\n",
    "            results.append(offset)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx = bisect.bisect_left(hits[0], next_offset)\n",
    "    return results\n",
    "    \n",
    "    \n",
    "def search(rindex, query):\n",
    "    doclists = [rindex[term] for term in query]\n",
    "    return list(isectn(*doclists))\n",
    "\n",
    "def parse_query(query):\n",
    "    return list(tokenize(query))\n",
    "\n",
    "def pretty_search(docs, rindex, query):\n",
    "    titles = {doc['docid']: doc['title'] for doc in docs}\n",
    "    results = search(rindex, parse_query(query))\n",
    "    return [titles[docid] for docid in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A few tests never hurt\"\"\"\n",
    "assert list(isect([0, 1, 2], [-1, 1, 3])) == [1]\n",
    "assert list(isectn([0, 1, 2], [1], [-1, 1, 2, 5])) == [1]\n",
    "rindex = {\n",
    "    'a': [0, 1, 2],\n",
    "    'b': [1],\n",
    "    'c': [1, 2],\n",
    "}\n",
    "assert search(rindex, ['a']) == [0, 1, 2]\n",
    "assert search(rindex, ['a', 'c']) == [1, 2], search(rindex, ['a', 'c'])\n",
    "assert search(rindex, ['a', 'b', 'c']) == [1]\n",
    "\n",
    "def test_exact_search():\n",
    "    sample = \"\"\"\n",
    "jack and jill ran up the hill\n",
    "but no one stopped to pay the bill\n",
    "up the hill up the jack the hill jack and jill\n",
    "\"\"\"\n",
    "    queries = [\n",
    "        'jack',\n",
    "        'jack and jill',\n",
    "        'no one stopped',\n",
    "        'up the hill',\n",
    "        'FOO BAR'\n",
    "    ]\n",
    "    tokens = list(tokenize(sample))\n",
    "    dindex = index_doc(mkdoc(content=sample))\n",
    "    for q in queries:\n",
    "        pq = parse_query(q)\n",
    "        expect = [offset for offset, _ in enumerate(tokens) if tokens[offset:offset+len(pq)] == pq]\n",
    "        assert exact_search(dindex, pq) == expect\n",
    "\n",
    "test_exact_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairest creatures: ['Sonnet I']\n",
      "her beauty: ['Sonnet IX', 'Sonnet XI', 'Sonnet XIX', 'Sonnet XXII', 'Sonnet XLI', 'Sonnet CXXVII']\n",
      "spring: ['Sonnet I', 'Sonnet LIII', 'Sonnet LXIII', 'Sonnet XCVIII', 'Sonnet CII']\n",
      "tender: ['Sonnet I', 'Sonnet XXII', 'Sonnet XLV', 'Sonnet LXXXIII', 'Sonnet CXX', 'Sonnet CXXVIII', 'Sonnet CXLI']\n",
      "thine own: ['Sonnet I', 'Sonnet II', 'Sonnet XIX', 'Sonnet XXXVIII', 'Sonnet XLIX', 'Sonnet LXIX', 'Sonnet CXLII']\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"fairest creatures\",\n",
    "    \"her beauty\",\n",
    "    \"spring\",\n",
    "    \"tender\",\n",
    "    \"thine own\",\n",
    "]\n",
    "for q in queries:\n",
    "    print(f\"{q}: {pretty_search(sonnet_docs, sonnet_rindex, q)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bisect import bisect_left\n",
    "\n",
    "class SortedIter:\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "        self.pos = 0\n",
    "    def __bool__(self):\n",
    "        return self.pos < len(self.items)\n",
    "    def __next__(self):\n",
    "        self.pos += 1\n",
    "    def seek(self, val):\n",
    "        self.pos = bisect_left(self.items, val)\n",
    "    def val(self, default=None):\n",
    "        return self.items[self.pos] if self.pos < len(self.items) else default\n",
    "\n",
    "def close_hits(doc_index, query, threshold=None):\n",
    "    if not query:\n",
    "        return\n",
    "    if len(query) == 1:\n",
    "        for _ in doc_index[query[0]]:\n",
    "            yield 0\n",
    "        return\n",
    "    iters = [SortedIter(doc_index[term])for term in query]\n",
    "    while iters[0]:\n",
    "        for i in range(1, len(iters)):\n",
    "            if not iters[i-1]:\n",
    "                continue\n",
    "            iters[i].seek(iters[i-1].val())\n",
    "            if not iters[i]:\n",
    "                continue\n",
    "            distance = iters[i].val() - iters[i-1].val()\n",
    "            if not threshold or distance <= threshold:\n",
    "                yield distance\n",
    "        next(iters[0])\n",
    "\n",
    "def bin_idx(bins, v):\n",
    "    return next(i for i in reversed(range(len(bins))) if v >= bins[i][0])\n",
    "\n",
    "def bin_weight(bins, v):\n",
    "    return bins[bin_idx(bins, v)][1]\n",
    "\n",
    "class ProximityScore:\n",
    "    def __init__(self, prox_bins=None, count_bins=None):\n",
    "        self.prox_bins = prox_bins   or [(0, 1.0), (1, 0.9), (2, 0.8), (3, 0.7), (4, 0.6), (5, 0.5), (10, 0.0)]\n",
    "        self.count_bins = count_bins or [(0, 0.0), (1, 0.5), (2, 0.75), (3, 0.85), (4, 0.95), (5, 1.0)]\n",
    "        self.counts = [0 for _ in prox_bins]\n",
    "    def add(self, distance):\n",
    "        idx = bin_idx(self.prox_bins, distance)\n",
    "        self.counts[idx] += 1\n",
    "    def finish(self):\n",
    "        prox_weights = [weight for prox, weight in self.prox_bins]\n",
    "        count_weights = [bin_weight(count_bins, count) for count in self.counts]\n",
    "        return np.dot(prox_weights, count_weights)\n",
    "        \n",
    "    \n",
    "def score_proximity(doc_index, query, prox_bins=None, count_bins=None):\n",
    "    score = ProximityScore(prox_bins, count_bins)\n",
    "    max_prox = max(prox for prox, weight in score.prox_bins)\n",
    "    for distance in close_hits(doc_index, query, threshold=max_prox):\n",
    "        score.add(distance)\n",
    "    return score.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_close_hits(content, query, expect):\n",
    "    doc = mkdoc(content=content)\n",
    "    query = parse_query(query)\n",
    "    distances = list(close_hits(index_doc(doc), query))\n",
    "    assert distances == expect, f\"{distances}, {expect}\"\n",
    "\n",
    "test_cases = [\n",
    "    (\"a a a\", \"a\", [0, 0, 0]),\n",
    "    (\"a b\", \"a b\", [1]),\n",
    "    (\"a b c\", \"a c\", [2]),\n",
    "    (\"a b c a d b\", \"a b c\", [1, 1, 2]),\n",
    "    ('a b d c a d d b', 'a b c', [1, 2, 3])\n",
    "]\n",
    "for args in test_cases:\n",
    "    check_close_hits(*args)\n",
    "    \n",
    "prox_bins  = [(0, 1), (2, 0.75), (3, 0.5)]\n",
    "count_bins = [(0, 0), (1, 0.5), (2, 1.0)]    \n",
    "    \n",
    "def check_proximity(content, query, expect):\n",
    "    doc = mkdoc(content=content)\n",
    "    query = parse_query(query)\n",
    "    score = score_proximity(index_doc(doc), query, prox_bins=prox_bins, count_bins=count_bins)\n",
    "    assert score == expect, f\"{score}, {expect}\"\n",
    "\n",
    "test_cases = [\n",
    "    ('a', 'a', 0.5),\n",
    "    ('a a', 'a', 1.0),\n",
    "    ('a b', 'a b', 0.5),\n",
    "    ('a c b', 'a b', 0.75 * 0.5),\n",
    "    ('a b d c a d d b', 'a b c', (1 * 0.5) + (0.75 * 0.5) + (0.5 * 0.5))\n",
    "]\n",
    "for args in test_cases:\n",
    "    check_proximity(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def docfreqs(docs):\n",
    "    docfreqs = Counter()\n",
    "    for doc in docs:\n",
    "        for term in set(tokenize(doc['content'])):\n",
    "            docfreqs[term] += 1\n",
    "    return docfreqs\n",
    "\n",
    "def termfreqs(doc):\n",
    "    return Counter(tokenize(doc['content']))\n",
    "    \n",
    "def tfidf(docs):\n",
    "    df = docfreqs(docs)\n",
    "    for doc in docs:\n",
    "        scores = {}\n",
    "        for term, tf in termfreqs(doc).items():\n",
    "            idf = math.log(len(docs) / df[term], 2)\n",
    "            tfidf = tf * idf\n",
    "            scores[term] = tfidf\n",
    "        yield scores\n",
    "        \n",
    "def tfidf_table(docs):\n",
    "    return {doc['docid']: scores for doc, scores in zip(docs, tfidf(docs))}\n",
    "\n",
    "def score_tfidf(doc, query):\n",
    "    pass  # TODO: implement\n",
    "   \n",
    "def score_freq(doc_index, query):\n",
    "    return sum(len(doc_index[term]) for term in query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
